{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe9vkEvFABbN"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train YOLOv8 Object Detection on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset)\n",
        "[![YouTube](https://badges.aleen42.com/src/youtube.svg)](https://youtu.be/wuZtUMEiKWY)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/ultralytics/ultralytics)\n",
        "\n",
        "Ultralytics YOLOv8 is a popular version of the YOLO (You Only Look Once) object detection and image segmentation model developed by Ultralytics. The YOLOv8 model is designed to be fast, accurate, and easy to use, making it an excellent choice for a wide range of object detection and image segmentation tasks. It can be trained on large datasets and is capable of running on a variety of hardware platforms, from CPUs to GPUs.\n",
        "\n",
        "## Disclaimer\n",
        "\n",
        "If you notice that our notebook behaves incorrectly - especially if you experience errors that prevent you from going through the tutorial - don't hesitate! Let us know and open an [issue](https://github.com/roboflow/notebooks/issues) on the Roboflow Notebooks repository.\n",
        "\n",
        "## Accompanying Blog Post\n",
        "\n",
        "We recommend that you follow along in this notebook while reading the accompanying [Blog Post](https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/).\n",
        "\n",
        "## Pro Tip: Use GPU Acceleration\n",
        "\n",
        "If you are running this notebook in Google Colab, navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`. This will ensure your notebook uses a GPU, which will significantly speed up model training times.\n",
        "\n",
        "## Steps in this Tutorial\n",
        "\n",
        "In this tutorial, we are going to cover:\n",
        "\n",
        "- Before you start\n",
        "- Install YOLOv8\n",
        "- CLI Basics\n",
        "- Inference with Pre-trained COCO Model\n",
        "- Roboflow Universe\n",
        "- Preparing a custom dataset\n",
        "- Custom Training\n",
        "- Validate Custom Model\n",
        "- Inference with Custom Model\n",
        "\n",
        "**Let's begin!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyRdDYkqAKN4"
      },
      "source": [
        "## Before you start\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y8cDtxLIBHgQ",
        "outputId": "b09010e9-073d-4f11-b8d4-3d9ee03dc154"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Nov 23 20:25:07 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P8              12W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CjpPg4mGKc1v",
        "outputId": "54485654-8502-4f3d-f00d-1036fccddb18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3C3EO_2zNChu"
      },
      "source": [
        "## Install YOLOv8\n",
        "\n",
        "YOLOv8 can be installed in two ways‚Ää-‚Ääfrom the source and via pip. This is because it is the first iteration of YOLO to have an official package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tdSMcABDNKW-",
        "outputId": "d1b95b8c-befc-45c7-ff86-f66caf46c053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ultralytics YOLOv8.2.103 üöÄ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Setup complete ‚úÖ (2 CPUs, 12.7 GB RAM, 32.5/112.6 GB disk)\n"
          ]
        }
      ],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install ultralytics==8.2.103 -q\n",
        "\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "VOEYrlBoP9-E"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "from IPython.display import display, Image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnnZSm5OQfPQ"
      },
      "source": [
        "## CLI Basics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K33S7zlkQku0"
      },
      "source": [
        "If you want to train, validate or run inference on models and don't need to make any modifications to the code, using YOLO command line interface is the easiest way to get started. Read more about CLI in [Ultralytics YOLO Docs](https://docs.ultralytics.com/usage/cli/).\n",
        "\n",
        "```\n",
        "yolo task=detect    mode=train    model=yolov8n.yaml      args...\n",
        "          classify       predict        yolov8n-cls.yaml  args...\n",
        "          segment        val            yolov8n-seg.yaml  args...\n",
        "                         export         yolov8n.pt        format=onnx  args...\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5RGYA6sPgEd"
      },
      "source": [
        "## Inference with Pre-trained COCO Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AFMBYQtMVL-B"
      },
      "source": [
        "### üêç Python SDK\n",
        "\n",
        "The simplest way of simply using YOLOv8 directly in a Python environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Rx9NWF-sVN6Y"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import csv\n",
        "#model = YOLO(f'{HOME}/yolov8n.pt')\n",
        "#results = model.predict(source='https://media.roboflow.com/notebooks/examples/dog.jpeg', conf=0.25)\n",
        "base_path = os.getcwd()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_list = [[]for _  in range(1000)]\n",
        "#ÂêàÊàêvideo_path\n",
        "video_path = ''\n",
        "output_folder_path = os.path.join(base_path, 'output')\n",
        "if not os.path.exists(output_folder_path):\n",
        "  print(output_folder_path)\n",
        "  os.makedirs(output_folder_path)"
      ],
      "metadata": {
        "id": "MZ5WQOwC1Fuk",
        "outputId": "bc7b9eb8-d7ec-4d4c-bb4e-67ea3a8d8ed9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "if not cap.isOpened():\n",
        "  print(f\"Error reading video file: {video_path}\")\n",
        "else:\n",
        "#output_videoÁöÑwidth, height, fpsË®≠ÂÆö\n",
        "  width, height, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
        "  #ÂêàÊàêoutput_video_path\n",
        "  if not os.path.exists(output_folder + \"/videoOutput\"):\n",
        "      os.makedirs(output_folder + \"/videoOutput\")\n",
        "    output_video_path = output_folder + \"/videoOutput/\"+filename[:-4]+\"_output.mp4\"\n",
        "\n",
        "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
        "    #Á¥ÄÈåÑËøΩËπ§Ë≥áË®ä\n",
        "    track_info = defaultdict(lambda: {\"frames\": [], \"bboxes\": []})\n",
        "    frame_num = 0\n",
        "    #ËÆÄÂπÄ\n",
        "\n",
        "    while cap.isOpened():\n",
        "        success, frame = cap.read()\n",
        "        if success:\n",
        "            frame_num += 1\n",
        "            #yoloV8ÂïüÂãï\n",
        "            results = model.track(frame, persist=True, tracker='bytetrack.yaml', classes = [2,7])\n",
        "            #Ë¶ÅÂÖàÁ¢∫Ë™çÂΩ±Áâá‰∏≠ÊòØÂê¶ÊúâÂÅµÊ∏¨Âà∞Áâ©‰ª∂\n",
        "            if results[0].boxes.id is not None:\n",
        "                #boxesÁÇ∫ÈÄô‰∏ÄÂπÄÊâÄÊúâbounding boxË≥áË®äÔºà‰∏≠ÂøÉÂ∫ßÊ®ô‰ª•Âèäw,h)ÂæóÈõÜÂêà\n",
        "                #track_idsÁÇ∫ÈÄô‰∏ÄÂπÄÊâÄÊúâidÂæóÈõÜÂêà\n",
        "\n",
        "                boxes = results[0].boxes.xywh.cpu()\n",
        "                track_ids = results[0].boxes.id.int().cpu().tolist()\n",
        "                #Â∞áÈ†êÊ∏¨ÁµêÊûúÂØ´ÂÖ•ÂΩ±ÁâáÔºàÂ∞±ÊòØÈÇ£‰∫õÊ°ÜÊ°ÜÔºâ\n",
        "                annotated_frame = results[0].plot()\n",
        "                cv2.putText(annotated_frame, str(frame_num), (20, 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255))\n",
        "\n",
        "                video_writer.write(annotated_frame)\n",
        "\n",
        "                #Â∞áboxesÂíåtrack_ids‰∏≠ÁöÑË≥áÊñôÊîæÂÖ•track_info\n",
        "                for box, track_id in zip(boxes, track_ids):\n",
        "                    x, y, w, h = box\n",
        "                    track_info[track_id][\"frames\"].append(frame_num)\n",
        "                    track_info[track_id][\"bboxes\"].append((x, y, w, h))\n",
        "                    #Ë®àÁÆóbounding boxÂ∑¶‰∏äËßíÂèäÂè≥‰∏ãËßíÂ∫ßÊ®ô‰ª•‰æõopencvÊà™Âúñ\n",
        "                    x1 = int(x - w / 2)\n",
        "                    y1 = int(y - h / 2)\n",
        "                    x2 = int(x1 + w)\n",
        "                    y2 = int(y1 + h)\n",
        "\n",
        "                    if cut_with_outside == 1:\n",
        "                        green_color = (0,255,0)\n",
        "                        roi = np.ones((frame.shape[0],frame.shape[1],frame.shape[2]), np.uint8) * 255\n",
        "                        roi[y1:y2, x1:x2] = frame[y1:y2, x1:x2]\n",
        "                        if y1 != 0:\n",
        "                            roi[y1-1, x1:x2] =  green_color\n",
        "                        if x1 != 0:\n",
        "                            roi[y1:y2, x1-1] =  green_color\n",
        "                        if y2 != frame.shape[0] - 1 and y2 != frame.shape[0]:\n",
        "                            roi[y2+1, x1:x2] =  green_color\n",
        "                        if x2 != frame.shape[1] - 1 and x2 != frame.shape[1]:\n",
        "                            roi[y1:y2, x2+1] =  green_color\n",
        "\n",
        "                    elif cut_with_outside == 2 :\n",
        "                        if x1 - 10 > 0:\n",
        "                            x1 -= 10\n",
        "                        else:\n",
        "                            x1 = 0\n",
        "                        if frame.shape[1] - x2 - 10 > 0:\n",
        "                            x2 += 10\n",
        "                        else:\n",
        "                            x2 = frame.shape[1] - 1\n",
        "\n",
        "                        roi = frame[y1:y2, x1:x2]\n",
        "                        roi = cv2.resize(roi, (224, 224))\n",
        "                    else:\n",
        "                        roi = frame[y1:y2, x1:x2]\n",
        "                    #ÂêàÊàêimgËº∏Âá∫path\n",
        "                    img_list[track_id].append(roi)\n",
        "            else:\n",
        "                continue\n",
        "        else:\n",
        "            break\n",
        "    model.predictor.trackers[0].reset()\n",
        "    cap.release()\n",
        "    video_writer.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    #ÂØ´txtÊ™î\n",
        "    output_file = os.path.join(info_folder, f\"{filename[:-4]}_track_info.txt\")\n",
        "    output_csvfile = os.path.join(info_folder, f\"{filename[:-4]}_track_info.csv\")\n",
        "    with open(output_file, \"w\") as f:\n",
        "        for track_id, info in track_info.items():\n",
        "            frames = info[\"frames\"]\n",
        "            bboxes = info[\"bboxes\"]\n",
        "            f.write(f\"Track ID: {track_id}\\n\")\n",
        "            f.write(\"Bounding Boxes:\\n\")\n",
        "            for bbox in bboxes:\n",
        "                x, y, w, h = bbox\n",
        "                f.write(f\"    Frame: {frames[bboxes.index(bbox)]}, Bounding Box: ({x}, {y}, {w}, {h})\\n\")\n",
        "\n",
        "    with open(output_csvfile, 'w', newline='') as csvoutput:\n",
        "        writer = csv.writer(csvoutput)\n",
        "        for track_id, info in track_info.items():\n",
        "            frames = info[\"frames\"]\n",
        "            bboxes = info[\"bboxes\"]\n",
        "            for bbox in bboxes:\n",
        "                x, y, w, h = bbox\n",
        "                frameNum= frames[bboxes.index(bbox)]\n",
        "                csvoutput.write(f'{track_id},{frameNum},{x:.17f},{y:.17f},{w},{h}\\n')\n",
        ""
      ],
      "metadata": {
        "id": "79hIrsjnuXit"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LDdByBbCwDy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Êñ∞Â¢ûÂçÄÊÆµ"
      ],
      "metadata": {
        "id": "ynrSXzj4v-tZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pigKhmbWv7q0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Yixi2htRvzxQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}